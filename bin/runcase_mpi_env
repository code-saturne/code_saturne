#!/bin/sh
#============================================================================
#
#     This file is part of the Code_Saturne Kernel, element of the
#     Code_Saturne CFD tool.
#
#     Copyright (C) 1998-2012 EDF S.A., France
#
#     contact: saturne-support@edf.fr
#
#     The Code_Saturne Kernel is free software; you can redistribute it
#     and/or modify it under the terms of the GNU General Public License
#     as published by the Free Software Foundation; either version 2 of
#     the License, or (at your option) any later version.
#
#     The Code_Saturne Kernel is distributed in the hope that it will be
#     useful, but WITHOUT ANY WARRANTY; without even the implied warranty
#     of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with the Code_Saturne Kernel; if not, write to the
#     Free Software Foundation, Inc.,
#     51 Franklin St, Fifth Floor,
#     Boston, MA  02110-1301  USA
#
#============================================================================
#
# Setup MPI environment, using the optional NUMBER_OF_PROCESSORS and
# PROCESSOR_LIST environment variables, as well as other system-specific
# environment variables.
#
if [ -f "$TMPDIR/machines" ] ; then     # Sun Grid Engine queueing system
   MPIHOSTS="$TMPDIR/machines"
elif [ ! -z "$PBS_NODEFILE" ] ; then    # PBS queueing system
   MPIHOSTS="$PBS_NODEFILE"
elif [ ! -z "$LSB_MCPU_HOSTS" ] ; then  # LSF queueing system
   \rm -f $RUN/hostsfile
   echo $LSB_MCPU_HOSTS | awk '{ for (ii = 1; ii < NF+1; ii+=2) {nn = ii+1; { for (jj = 1; jj < $nn; jj++ ) print $ii }}}' >> $RUN/hostsfile
   MPIHOSTS="$RUN/hostsfile"
elif [ ! -z "$LSB_HOSTS" ] ; then       # LSF queueing system
   \rm -f $RUN/hostsfile
   echo $LSB_HOSTS | awk '{ for (ii = 1; ii < NF+1; ++ii) print $ii }' >> $RUN/hostsfile
   MPIHOSTS="$RUN/hostsfile"
elif [ ! -z "$LOADL_HOSTFILE" ] ; then  # Load Leveler
   MPIHOSTS="$LOADL_HOSTFILE"
elif [ ! -z "$SLURM_HOSTFILE" ] ; then  # SLURM
   MPIHOSTS="$SLURM_HOSTFILE"
elif [ ! -z "$PROCESSOR_LIST" ] ; then  # No resource manager
   \rm -f $RUN/hostsfile
   localproc=`hostname`
   echo $PROCESSOR_LIST | grep -q $localproc || PROCESSOR_LIST="$localproc & $PROCESSOR_LIST"
   echo $PROCESSOR_LIST | awk  -F\& '{ for (ii = 1; ii < NF+1; ++ii) print $ii }' >> $RUN/hostsfile
   MPIHOSTS="$RUN/hostsfile"
fi
# Check for the number of processors
if [ -z  "${NUMBER_OF_PROCESSORS}" ] ; then
  if [ ! -z  "${NSLOTS}" ] ; then
    # Sun Grid Engine
    NUMBER_OF_PROCESSORS=$NSLOTS
  elif [ ! -z "${SLURM_NPROCS}" ] ; then
    # SLURM
    NUMBER_OF_PROCESSORS=$SLURM_NPROCS
  elif [ ! -z "$SLURM_BG_NUM_NODES" ] ; then
    # Blue Gene using SLURM
    NUMBER_OF_PROCESSORS=`python -c "
import os
s = os.getenv('SLURM_TASKS_PER_NODE')
n_procs = 0
for s0 in s.split(','):
    i = s0.find('(')
    if i > -1:
        n_procs += int(s0[0:i])*int(s0[i+2:-1])
    else:
        n_procs += int(s0)
print n_procs"`
  elif [ ! -z "${SLURM_NNODES}" ] ; then
    # SLURM
    NUMBER_OF_PROCESSORS=`srun hostname -s | wc -w`
  elif [ ! -z "${LSB_MCPU_HOSTS}" ] ; then
    # LSF queueing system, using LSB_MCPU_HOSTS
    NUMBER_OF_PROCESSORS=`echo $LSB_MCPU_HOSTS | awk 'BEGIN {CPUS=0} {for (ii=2; ii <= NF;ii+=2) CPUS+=$ii} END {print(CPUS)}'`
  elif [ ! -z "${LSB_HOSTS}" ] ; then
    # LSF queueing system, using LSB_HOSTS
    NUMBER_OF_PROCESSORS=`echo $LSB_HOSTS | wc -w`
  elif [ ! -z  "${MPIHOSTS}" ] ; then
    # PBS queueing system
    NUMBER_OF_PROCESSORS=`cat $MPIHOSTS | wc -l`
  elif [ ! -z "${LOADL_TOTAL_TASKS}" ] ; then
    # Load Leveler
    NUMBER_OF_PROCESSORS=$LOADL_TOTAL_TASKS
  elif [ ! -z "${LOADL_HOSTFILE}" ] ; then
    # Load Leveler
    NUMBER_OF_PROCESSORS=`echo $LOADL_HOSTFILE | wc -w`
  else
    NUMBER_OF_PROCESSORS=1
  fi
fi
#
# Check the number of nodes if possible
NUMBER_OF_NODES=1
if [ ! -z  "${MPIHOSTS}" ] ; then
  NUMBER_OF_NODES=`sort $MPIHOSTS | sed -e 's/:/ /' | cut -f 1 -d" " | uniq | wc -l`
fi
#
if [ ! -z "$MPIHOSTS" ] ; then
  echo "Total number of processors: $NUMBER_OF_PROCESSORS"
fi
#
# System-related parameters
#
if [ $NUMBER_OF_PROCESSORS -gt 1 ] ; then
#
# Temporarily set the architecture name (for EDF purposes)
  NOM_ARCH=`uname -s`
# Default values
  MPIBOOT=""
  MPIHALT=""
  MPIRUN="mpirun -np ${NUMBER_OF_PROCESSORS}"
  if [ -d "$CS_MPI_PATH" ] ; then
    export PATH=${CS_MPI_PATH}:$PATH
  fi
#
  if [ ! -z "$SLURM_BG_NUM_NODES" ] ; then
    MPIRUN="srun --ntasks=${NUMBER_OF_PROCESSORS}"

  elif [ "$NOM_ARCH" = "AIX" ] ; then
    MPIRUN="poe"
#
  else
    if [ -d "$CS_MPI_PATH" ] ; then
      if [ -f "${CS_MPI_PATH}/mpdboot" ] ; then        # For MPICH2
        MPIBOOT="${CS_MPI_PATH}/mpdboot"
        MPIHALT="${CS_MPI_PATH}/mpdallexit"
        MPIRUN="${CS_MPI_PATH}/mpiexec -n ${NUMBER_OF_PROCESSORS}"
        if [ ! -z "$MPIHOSTS" ] ; then
          MPIBOOT="$MPIBOOT -n ${NUMBER_OF_NODES} -v -f $MPIHOSTS"
        fi
      elif [ -f "${CS_MPI_PATH}/ompi_info" ] ; then    # For Open MPI
        MPIRUN="${CS_MPI_PATH}/mpiexec -n ${NUMBER_OF_PROCESSORS}"
        if [ ! -z "$MPIHOSTS" ] ; then
          MPIRUN="$MPIRUN -machinefile $MPIHOSTS"
        fi
      elif [ -f "${CS_MPI_PATH}/lamboot" ] ; then      # For LAM MPI
        MPIBOOT="${CS_MPI_PATH}/lamboot"
        MPIHALT="${CS_MPI_PATH}/lamhalt"
        MPIRUN="${CS_MPI_PATH}/mpirun -np ${NUMBER_OF_PROCESSORS}"
        if [ ! -z "$MPIHOSTS" ] ; then
          MPIBOOT="$MPIBOOT -v $MPIHOSTS"
          MPIHALT="$MPIHALT -v $MPIHOSTS"
        fi
      elif [ -f "${CS_MPI_PATH}/mpichversion" ] ; then # For MPICH
        MPIRUN="${CS_MPI_PATH}/mpirun -np ${NUMBER_OF_PROCESSORS}"
        if [ ! -z "$TMPDIR/machines" ] ; then          # For Sun Grid Engine
          MPIRUN="$MPIRUN -machinefile $TMPDIR/machines"
        elif [ ! -z "$MPIHOSTS" ] ; then
          MPIRUN="$MPIRUN -machinefile $MPIHOSTS"
        fi
      fi
    fi
  fi
#
#   In serial mode
#
else
#
  MPIRUN=""
  MPIBOOT=""
  MPIHALT=""
fi
#

